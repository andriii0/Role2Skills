{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![job image](job.jpg \"Job image\")",
   "id": "d5ad223eb83fffcd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Role2Skills Project\n",
    "The idea is a system which extracts skills from job descriptions and uses machine learning to cluster and classify job roles, revealing which skill combinations define different professions and what competencies are most demanded on the job market."
   ],
   "id": "ba12d481a3f675a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"axes.facecolor\": \"white\",\n",
    "    \"savefig.facecolor\": \"white\",\n",
    "    \"axes.edgecolor\": \"black\",\n",
    "    \"text.color\": \"black\",\n",
    "    \"axes.labelcolor\": \"black\",\n",
    "    \"xtick.color\": \"black\",\n",
    "    \"ytick.color\": \"black\",\n",
    "    \"grid.color\": \"0.85\",\n",
    "})\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.decomposition import PCA"
   ],
   "id": "3f5adb996528becd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset imports",
   "id": "dc3735b6c0dd7924"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vacancy dataset imported from Kaggle: https://www.kaggle.com/code/mpwolke/yandex-jobs and translated into english by me",
   "id": "268a9fac1d13b5cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load data\n",
    "CSV_PATH = \"data/yandex vacancies eng.csv\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "df.head()\n"
   ],
   "id": "86ae8c6a3378ddf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Skills dataset in order to expand skill dictionary with more skills. Imported from Kaggle: https://www.kaggle.com/datasets/zamamahmed211/skills and converted into csv",
   "id": "4145975a933beecd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dataset full of different skills (May be useful)\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "SKILLS_PATH = \"data/skills_dataset.csv\"\n",
    "skills_df = pd.read_csv(SKILLS_PATH)\n",
    "\n",
    "skills_df.head()"
   ],
   "id": "c20fdc05e5bce570",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Turning vacancy text fields into a single text block for skill extraction and model training",
   "id": "b0809cc5a4680d34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Basic cleaning (safe columns)\n",
    "\n",
    "for col in [\"Requirements\", \"Description\", \"Pluses\", \"Hashtags\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(\"\").astype(str)\n",
    "    else:\n",
    "        df[col] = \"\"\n",
    "\n",
    "# Combine text fields for skill extraction\n",
    "# Use set to remove duplicated text blocks\n",
    "\n",
    "def combine_unique_text(row):\n",
    "    parts = {\n",
    "        row[\"Requirements\"].strip().lower(),\n",
    "        row[\"Description\"].strip().lower(),\n",
    "        row[\"Pluses\"].strip().lower(),\n",
    "        row[\"Hashtags\"].strip().lower(),\n",
    "    }\n",
    "    parts.discard(\"\")  # remove empty strings\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "df[\"text\"] = df.apply(combine_unique_text, axis=1)"
   ],
   "id": "123b34c3872c5fcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Regex-based skill extraction to identify skills mentioned in job descriptions",
   "id": "abd97e26b93e5c6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Key = canonical name, value = regex that matches common variants\n",
    "skills_regex = {\n",
    "    # Languages\n",
    "    \"python\": r\"\\bpython\\b\",\n",
    "    \"java\": r\"\\bjava\\b\",\n",
    "    \"kotlin\": r\"\\bkotlin\\b\",\n",
    "    \"swift\": r\"\\bswift\\b\",\n",
    "    \"go\": r\"\\bgolang\\b|\\bgo\\b\",\n",
    "    \"ruby\": r\"\\bruby\\b\",\n",
    "    \"php\": r\"\\bphp\\b\",\n",
    "    \"scala\": r\"\\bscala\\b\",\n",
    "    \"c\": r\"\\bc\\b(?!\\+|\\#)\",               # tries to avoid catching c++ / c#\n",
    "    \"c++\": r\"\\bc\\+\\+\\b|\\bcpp\\b\",\n",
    "    \"c#\": r\"\\bc\\#\\b|c\\s*sharp\",\n",
    "    \".net\": r\"\\.net\\b|dotnet\",\n",
    "    \"javascript\": r\"\\bjavascript\\b|\\bjs\\b\",\n",
    "    \"typescript\": r\"\\btypescript\\b|\\bts\\b\",\n",
    "\n",
    "    # Web / frontend\n",
    "    \"html\": r\"\\bhtml\\b\",\n",
    "    \"css\": r\"\\bcss\\b\",\n",
    "    \"sass\": r\"\\bsass\\b|\\bscss\\b\",\n",
    "    \"react\": r\"\\breact\\b\",\n",
    "    \"next.js\": r\"\\bnext\\.?js\\b\",\n",
    "    \"vue\": r\"\\bvue\\b|\\bvue\\.?js\\b\",\n",
    "    \"angular\": r\"\\bangular\\b\",\n",
    "    \"redux\": r\"\\bredux\\b\",\n",
    "\n",
    "    # Backend / frameworks\n",
    "    \"node.js\": r\"\\bnode\\.?js\\b|\\bnodejs\\b\",\n",
    "    \"express\": r\"\\bexpress\\b\",\n",
    "    \"nestjs\": r\"\\bnest\\.?js\\b|\\bnestjs\\b\",\n",
    "    \"spring\": r\"\\bspring\\b\",\n",
    "    \"spring boot\": r\"\\bspring\\s*boot\\b\",\n",
    "    \"django\": r\"\\bdjango\\b\",\n",
    "    \"flask\": r\"\\bflask\\b\",\n",
    "    \"fastapi\": r\"\\bfastapi\\b\",\n",
    "    \"laravel\": r\"\\blaravel\\b\",\n",
    "    \"rails\": r\"\\brails\\b|ruby on rails\",\n",
    "\n",
    "    # Databases\n",
    "    \"sql\": r\"\\bsql\\b\",\n",
    "    \"postgresql\": r\"\\bpostgres(?:ql)?\\b\",\n",
    "    \"mysql\": r\"\\bmysql\\b\",\n",
    "    \"mongodb\": r\"\\bmongo(?:db)?\\b\",\n",
    "    \"redis\": r\"\\bredis\\b\",\n",
    "    \"elasticsearch\": r\"\\belasticsearch\\b|\\belk\\b\",\n",
    "\n",
    "    # DevOps / cloud\n",
    "    \"linux\": r\"\\blinux\\b\",\n",
    "    \"git\": r\"\\bgit\\b\",\n",
    "    \"docker\": r\"\\bdocker\\b\",\n",
    "    \"kubernetes\": r\"\\bkubernetes\\b|\\bk8s\\b\",\n",
    "    \"terraform\": r\"\\bterraform\\b\",\n",
    "    \"ansible\": r\"\\bansible\\b\",\n",
    "    \"ci/cd\": r\"\\bci\\/cd\\b|\\bcicd\\b|\\bcontinuous integration\\b\",\n",
    "    \"aws\": r\"\\baws\\b|amazon web services\",\n",
    "    \"gcp\": r\"\\bgcp\\b|google cloud\",\n",
    "    \"azure\": r\"\\bazure\\b|microsoft azure\",\n",
    "\n",
    "    # APIs / messaging\n",
    "    \"rest\": r\"\\brest\\b|\\brestful\\b\",\n",
    "    \"graphql\": r\"\\bgraphql\\b\",\n",
    "    \"grpc\": r\"\\bgrpc\\b\",\n",
    "    \"kafka\": r\"\\bkafka\\b\",\n",
    "    \"rabbitmq\": r\"\\brabbitmq\\b\",\n",
    "\n",
    "    # Data / ML\n",
    "    \"pandas\": r\"\\bpandas\\b\",\n",
    "    \"numpy\": r\"\\bnumpy\\b\",\n",
    "    \"scikit-learn\": r\"scikit[-\\s]?learn|\\bsklearn\\b\",\n",
    "    \"pytorch\": r\"\\bpytorch\\b\",\n",
    "    \"tensorflow\": r\"\\btensorflow\\b\",\n",
    "    \"spark\": r\"\\bspark\\b|\\bpyspark\\b\",\n",
    "    \"airflow\": r\"\\bairflow\\b\",\n",
    "}\n",
    "\n",
    "# IMPORTANT: keep a clean copy of manual dict\n",
    "skills_regex_manual = dict(skills_regex)\n",
    "\n",
    "print(\"Manual skills_regex size:\", len(skills_regex_manual))"
   ],
   "id": "15ddb2e79e2311dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Normalization of extracted skills to canonical forms to handle synonyms and variations",
   "id": "d859a2fcd7c0bd73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Skill normalization: config & helpers ---\n",
    "\n",
    "SKILL_CANONICAL_MAP = {\n",
    "    # JS ecosystem\n",
    "    \"node.js\": \"nodejs\",\n",
    "    \"nodejs\": \"nodejs\",\n",
    "    \"node_js\": \"nodejs\",\n",
    "    \"express\": \"express\",\n",
    "    \"expressjs\": \"express\",\n",
    "\n",
    "    # CI/CD variants\n",
    "    \"ci/cd\": \"ci_cd\",\n",
    "    \"cicd\": \"ci_cd\",\n",
    "    \"ci-cd\": \"ci_cd\",\n",
    "    \"ci cd\": \"ci_cd\",\n",
    "\n",
    "    # Spark variants\n",
    "    \"pyspark\": \"spark\",\n",
    "    \"apache spark\": \"spark\",\n",
    "    \"spark\": \"spark\",\n",
    "\n",
    "    # DB variants\n",
    "    \"postgres\": \"postgresql\",\n",
    "    \"postgresql\": \"postgresql\",\n",
    "    \"postgre\": \"postgresql\",\n",
    "\n",
    "    # ML libs\n",
    "    \"scikit-learn\": \"sklearn\",\n",
    "    \"scikit learn\": \"sklearn\",\n",
    "    \"sklearn\": \"sklearn\",\n",
    "\n",
    "    # C-family\n",
    "    \"c\": \"c\",\n",
    "    \"c++\": \"cpp\",\n",
    "    \"cpp\": \"cpp\",\n",
    "    \"c#\": \"csharp\",\n",
    "    \"csharp\": \"csharp\",\n",
    "\n",
    "    # Go\n",
    "    \"golang\": \"go\",\n",
    "    \"go\": \"go\",\n",
    "    \"go lang\": \"go\",\n",
    "\n",
    "    # Common skills\n",
    "    \"python\": \"python\",\n",
    "    \"java\": \"java\",\n",
    "    \"kotlin\": \"kotlin\",\n",
    "    \"javascript\": \"javascript\",\n",
    "    \"js\": \"javascript\",\n",
    "    \"typescript\": \"typescript\",\n",
    "    \"ts\": \"typescript\",\n",
    "    \"react\": \"react\",\n",
    "    \"reactjs\": \"react\",\n",
    "    \"redux\": \"redux\",\n",
    "    \"html\": \"html\",\n",
    "    \"css\": \"css\",\n",
    "    \"git\": \"git\",\n",
    "    \"linux\": \"linux\",\n",
    "    \"docker\": \"docker\",\n",
    "    \"sql\": \"sql\",\n",
    "    \"mysql\": \"mysql\",\n",
    "    \"tensorflow\": \"tensorflow\",\n",
    "    \"tf\": \"tensorflow\",\n",
    "    \"pytorch\": \"pytorch\",\n",
    "    \"torch\": \"pytorch\",\n",
    "}\n",
    "\n",
    "DANGEROUS_CANONICAL = {\"c\", \"go\"}\n",
    "\n",
    "def _valid_dangerous(canonical: str, text: str) -> bool:\n",
    "    t = (text or \"\").lower()\n",
    "\n",
    "    if canonical == \"go\":\n",
    "        return (\"golang\" in t) or (\"go lang\" in t)\n",
    "\n",
    "    if canonical == \"c\":\n",
    "        return (\n",
    "            \"ansi c\" in t\n",
    "            or \"c language\" in t\n",
    "            or \"embedded c\" in t\n",
    "            or \"iso c\" in t\n",
    "        )\n",
    "\n",
    "    return True\n",
    "\n",
    "def normalize_skills(raw_skills: list[str], text: str) -> list[str]:\n",
    "    norm = []\n",
    "    for s in raw_skills:\n",
    "        s0 = str(s).strip().lower()\n",
    "        canonical = SKILL_CANONICAL_MAP.get(s0, s0)\n",
    "\n",
    "        if canonical in DANGEROUS_CANONICAL and not _valid_dangerous(canonical, text):\n",
    "            continue\n",
    "\n",
    "        norm.append(canonical)\n",
    "\n",
    "    out = set(norm)\n",
    "\n",
    "    if (\"cpp\" in out) or (\"csharp\" in out):\n",
    "        out.discard(\"c\")\n",
    "\n",
    "    return sorted(out)\n",
    "\n",
    "print(\"Normalization ready.\")\n"
   ],
   "id": "4f240d72265f01c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Process Kaggle skills dataset ---\n",
    "\n",
    "raw = (\n",
    "    skills_df[\"Skills\"]\n",
    "    .astype(str)\n",
    "    .str.replace('\"', '', regex=False)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "raw = raw[raw.notna()]\n",
    "raw = raw[~raw.isin([\"\", \"nan\", \"none\", \"null\"])]\n",
    "raw = raw[raw.str.len() >= 2]\n",
    "\n",
    "# Noise filtering (non-capturing group to avoid pandas warning)\n",
    "BAD_PATTERNS = [\n",
    "    r\"[^\\x00-\\x7F]\",  # non-latin\n",
    "    r\"\\d\",            # digits\n",
    "    r\"\\b(?:pay|salary|equity|insurance|travel|student|retention|payback|bullet)\\b\",\n",
    "]\n",
    "bad_re = re.compile(\"|\".join(BAD_PATTERNS))\n",
    "\n",
    "raw = raw[~raw.str.contains(bad_re, regex=True)]\n",
    "\n",
    "raw_unique = sorted(set(raw.tolist()))\n",
    "\n",
    "# Normalize Kaggle skills using normalization\n",
    "norm_skills = []\n",
    "for s in raw_unique:\n",
    "    out = normalize_skills([s], text=s)  # context = itself\n",
    "    if out:\n",
    "        norm_skills.extend(out)\n",
    "\n",
    "norm_unique = sorted(set(norm_skills))\n",
    "\n",
    "def _safe_regex(skill: str) -> str:\n",
    "    return rf\"(?<!\\w){re.escape(skill)}(?!\\w)\"\n",
    "\n",
    "# --- FILTER by vacancy dataset frequency (document frequency) ---\n",
    "MIN_DOC_FREQ = 5  # tune: 3, 5, 10\n",
    "\n",
    "# compile patterns once for speed\n",
    "cand_compiled = {s: re.compile(_safe_regex(s), flags=re.IGNORECASE) for s in norm_unique}\n",
    "\n",
    "doc_freq = {s: 0 for s in norm_unique}\n",
    "texts = df[\"text\"].astype(str).tolist()\n",
    "\n",
    "for text in texts:\n",
    "    for s, pat in cand_compiled.items():\n",
    "        if pat.search(text):\n",
    "            doc_freq[s] += 1\n",
    "\n",
    "kaggle_kept = sorted([s for s, c in doc_freq.items() if c >= MIN_DOC_FREQ])\n",
    "\n",
    "# RESET to manual dict, then add only kept Kaggle skills\n",
    "skills_regex = dict(skills_regex_manual)\n",
    "\n",
    "added = 0\n",
    "for s in kaggle_kept:\n",
    "    if s not in skills_regex:\n",
    "        skills_regex[s] = _safe_regex(s)\n",
    "        added += 1\n",
    "\n",
    "print(f\"[skills_dataset] Raw unique: {len(raw_unique)}\")\n",
    "print(f\"[skills_dataset] Normalized unique: {len(norm_unique)}\")\n",
    "print(f\"[skills_dataset] Kept by df >= {MIN_DOC_FREQ}: {len(kaggle_kept)}\")\n",
    "print(f\"[skills_dataset] Added to skills_regex: {added}\")\n",
    "print(f\"[skills_dataset] Final skills_regex size: {len(skills_regex)}\")\n"
   ],
   "id": "d13f6a1f0c429e75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Skill extraction (regex)\n",
    "def extract_skills(text: str) -> list[str]:\n",
    "    found = []\n",
    "    for skill, pattern in skills_regex.items():\n",
    "        if re.search(pattern, text, flags=re.IGNORECASE):\n",
    "            found.append(skill)\n",
    "    return found\n",
    "\n",
    "# 1) raw extraction\n",
    "df[\"skills_raw\"] = df[\"text\"].apply(extract_skills)\n",
    "\n",
    "# 2) canonical normalization + dedup per vacancy\n",
    "df[\"skills\"] = df.apply(lambda r: normalize_skills(r[\"skills_raw\"], r[\"text\"]), axis=1)\n",
    "\n",
    "df[\"skills_count\"] = df[\"skills\"].apply(len)\n",
    "\n",
    "df[[\"skills\", \"skills_count\"]].head()"
   ],
   "id": "4d21a5539089d42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plots and analysis of extracted skills",
   "id": "ae50e8acf4c59840"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot helpers\n",
    "def plot_hist(series, title, xlabel, ylabel, bins=30):\n",
    "    plt.figure()\n",
    "    plt.hist(series.dropna(), bins=bins)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_bar(series, title, xlabel, ylabel=\"Count\"):\n",
    "    counts = series.value_counts().sort_index()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.bar(counts.index, counts.values)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ],
   "id": "b88edc021067d0b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Skill frequency\n",
    "all_skills = [s for row in df[\"skills\"] for s in row]\n",
    "skill_freq = Counter(all_skills)\n",
    "\n",
    "top = skill_freq.most_common(15)\n",
    "labels = [k for k, _ in top]\n",
    "values = [v for _, v in top]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(labels, values)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Top skills\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ],
   "id": "cb4ffa0b82b7bdc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Some skills like Python and SQL appear much more often than others. This shows skill imbalance in the data and why normalization is needed.",
   "id": "856bcf44f67d8e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Skills per job\n",
    "plot_bar(\n",
    "    df[\"skills_count\"],\n",
    "    title=\"Number of skills per job vacancy\",\n",
    "    xlabel=\"Number of skills mentioned in a job vacancy\",\n",
    "    ylabel=\"Number of job vacancies\"\n",
    ")\n",
    "\n",
    "\n",
    "# for example 150 vacancies require 2 skills, 60 vacancies require 4 skills, etc."
   ],
   "id": "52210434307487bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This distribution shows that most job vacancies mention only a small number of skills.\n",
    "This suggests that job descriptions are usually focused, which makes a dictionary-based skill extraction approach suitable."
   ],
   "id": "61a4a5c7135060d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_skills = [s for skills in df[\"skills\"] for s in skills]\n",
    "skill_freq = Counter(all_skills)\n",
    "\n",
    "skill_freq_series = pd.Series(skill_freq.values())\n",
    "\n",
    "plot_hist(\n",
    "    series=skill_freq_series,\n",
    "    title=\"Distribution of skill frequencies\",\n",
    "    xlabel=\"Number of vacancies mentioning a skill\",\n",
    "    ylabel=\"Number of skills\",\n",
    "    bins=40\n",
    ")\n",
    "\n",
    "# for example 25 skills are mentioned in ~7 vacancies, 1 skill is mentioned in ~280 vacancies, etc\n",
    "# for example sql or python are mentioned in ~280 vacancies, while some rare skills (around 28 of them) are mentioned in only several vacancies"
   ],
   "id": "9d5fcf6d9f886999",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This distribution shows that a few skills (for example, Python or SQL) appear very often, while most skills are mentioned only in a small number of vacancies.\n",
    "This indicates an imbalanced skill distribution and explains why skill normalization is needed."
   ],
   "id": "a168e28cb70b13cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filter skills by document frequency\n",
    "N = len(df)\n",
    "skill_doc_freq = Counter()\n",
    "\n",
    "for row in df[\"skills\"]:\n",
    "    for s in set(row):\n",
    "        skill_doc_freq[s] += 1\n",
    "\n",
    "min_df = 5 # previously was max(2, int(0.01 * N)); set to 5 because skipped a lot of useful skills\n",
    "max_df = int(0.7 * N)\n",
    "\n",
    "kept_skills = [\n",
    "    s for s, c in skill_doc_freq.items()\n",
    "    if min_df <= c <= max_df\n",
    "]\n",
    "\n",
    "kept_skills\n"
   ],
   "id": "de0bc9463e7a2834",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "kept_skills = sorted(kept_skills)\n",
    "\n",
    "def filter_skills(skills):\n",
    "    return [s for s in skills if s in kept_skills]\n",
    "\n",
    "df[\"skills_filtered\"] = df[\"skills\"].apply(filter_skills)\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=kept_skills)\n",
    "X = mlb.fit_transform(df[\"skills_filtered\"])\n",
    "\n",
    "print(\"X shape:\", X.shape)\n"
   ],
   "id": "93204bc3ecf3bf1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# compute co-occurrence matrix\n",
    "co = (X.T @ X).astype(int)\n",
    "np.fill_diagonal(co, 0)\n",
    "\n",
    "# choose top skills by frequency\n",
    "skill_freq = Counter()\n",
    "\n",
    "for row in df[\"skills_filtered\"]:\n",
    "    for s in row:\n",
    "        skill_freq[s] += 1\n",
    "\n",
    "TOP_N = 20\n",
    "top_skills = [s for s, _ in skill_freq.most_common(TOP_N)]\n",
    "\n",
    "# indices of these skills in kept_skills / X\n",
    "idx = [kept_skills.index(s) for s in top_skills]\n",
    "\n",
    "co_top = co[np.ix_(idx, idx)]\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(9, 7))\n",
    "plt.imshow(co_top, aspect=\"auto\")\n",
    "plt.xticks(range(len(top_skills)), top_skills, rotation=90)\n",
    "plt.yticks(range(len(top_skills)), top_skills)\n",
    "plt.title(\"Skill co-occurrence (top skills by frequency)\")\n",
    "plt.colorbar(label=\"Number of vacancies\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "2f2c7a362dba3c66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The analysis shows that most job vacancies list only a few skills, while some list many, creating a long-tail distribution. Common skills appear much more often and frequently occur together, which reflects typical technology stacks in job roles.",
   "id": "f5bbb47f21d2ef62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Searching for data clusters\n",
    "HDBSCAN was used to cluster job vacancies based on their skill vectors in order to identify groups of similar job roles. The algorithm automatically finds dense clusters of vacancies with similar skill requirements and assigns unclear or rare cases to noise. For each cluster, the most common skills were extracted to interpret and describe the corresponding job role."
   ],
   "id": "7da2ce5fa194580c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=10,      # role minimal size\n",
    "    min_samples=5,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\"\n",
    ")\n",
    "\n",
    "labels = clusterer.fit_predict(X)\n",
    "\n",
    "df[\"cluster\"] = labels\n"
   ],
   "id": "d44f2b758b85ce4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df[\"cluster\"].value_counts().sort_index()",
   "id": "e5d428f48d222311",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "cluster_skills = []\n",
    "\n",
    "for cl in sorted(df[\"cluster\"].unique()):\n",
    "    if cl == -1:\n",
    "        continue\n",
    "\n",
    "    subset = df[df[\"cluster\"] == cl]\n",
    "    freq = Counter([s for row in subset[\"skills_filtered\"] for s in row])\n",
    "\n",
    "    cluster_skills.append({\n",
    "        \"cluster\": cl,\n",
    "        \"size\": len(subset),\n",
    "        \"top_skills\": \", \".join([s for s, _ in freq.most_common(10)])\n",
    "    })\n",
    "\n",
    "pd.DataFrame(cluster_skills).sort_values(\"size\", ascending=False)"
   ],
   "id": "5e4cc7af593d166",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "HDBSCAN labels dense groups of similar vacancies as clusters, while assigning less common or ambiguous vacancies to noise (cluster -1).\n",
    "A minimum cluster size of 10 was used, meaning that only job role types with at least 10 similar vacancies were considered stable clusters."
   ],
   "id": "a892a338e4fdd9d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df[df[\"cluster\"] == 0][[\"Header\", \"skills\"]].head(5)",
   "id": "15da323dcf5217e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using HDBSCAN, similar job vacancies were grouped into clusters based on shared skills. The clusters represent common job roles such as frontend, backend, data, and DevOps, while less clear vacancies were labeled as noise. Overall, the results show that job roles can be identified using skill-based representations.",
   "id": "7cadb21024f13350"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Classificator model\n",
    "A multi-label classification model was trained to predict required skills from job vacancy text. TF-IDF features were extracted from the vacancy text and a One-Vs-Rest logistic regression classifier was used to predict multiple skills for each vacancy. The trained model can take a job description as input and return the most likely required skills."
   ],
   "id": "67533db42c00a493"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "df_train = df[df[\"skills_filtered\"].apply(len) > 0].copy()\n",
    "X_text = df_train[\"text\"].fillna(\"\").astype(str)\n",
    "y_lists = df_train[\"skills_filtered\"]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(y_lists)\n",
    "\n",
    "X_train, _, Y_train, _ = train_test_split(X_text, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.9,\n",
    "        sublinear_tf=True,\n",
    "        max_features=200_000\n",
    "    )),\n",
    "    (\"ovr\", OneVsRestClassifier(\n",
    "        LogisticRegression(\n",
    "            solver=\"liblinear\",\n",
    "            max_iter=2000,\n",
    "            class_weight=\"balanced\"\n",
    "        )\n",
    "    ))\n",
    "])\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "def predict_skills(text: str, top_k: int = 10):\n",
    "    p = clf.predict_proba([str(text)])[0]\n",
    "    idx = np.argsort(p)[::-1][:top_k]\n",
    "    return [mlb.classes_[i] for i in idx]\n"
   ],
   "id": "3ecdac9dfafab69a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Lets check the model in action",
   "id": "6dec92ec30b316f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "predict_skills(\"Flutter developer\", top_k=10)",
   "id": "1f72f48603a23b8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The trained model can predict relevant skills for new job descriptions by analyzing the text and outputting the most probable skills based on learned patterns from the training data.",
   "id": "7426fa81d35d3cfe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
